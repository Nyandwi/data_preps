{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "#  A Handy Notes about Feature Scaling",
   "metadata": {
    "id": "73eca2c6",
    "cell_id": "00000-64bbb88d-d1a3-4cf2-920e-47a736008e45",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Machine Learning models are very selective about the type and range of values that have to go for their input in order do what they do. \n\nWith the exception of decision trees, most ML models will expect you to scale the input features. \n\nWhat is feature scaling?\n\nFeature scaling is nothing other than transforming the numerical features into a small range of values. In this notebook, we will see the following scaling technique:\n\n* Normalization\n* Standardizatiom\n* Robust Scaling\n\nYou will see Normalization and Standardization used interchangeably, but they are quite different and they are suited for different purposes. ",
   "metadata": {
    "id": "f755b200",
    "cell_id": "00001-354084fe-fcde-4c26-9133-ee7e4c54a101",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Normalization",
   "metadata": {
    "id": "b21c5e41",
    "cell_id": "00002-0f1cafdd-2d76-4913-83d6-95e06321f1ef",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Normalization is a scaling techniques that transform the numerical feature to the range of values between 0 and 1. \n\nHere is a formula that is followed when normalizing the data. $Xmin$ is the minimum value of feature X, and $Xmax$ is the maximum value of X. \n\n",
   "metadata": {
    "id": "3cbaae7d",
    "cell_id": "00003-719b0dc2-e034-4dae-a6d6-90f1806512a0",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "$$\nXnorm = \\frac {X-Xmin} {Xmax-Xmin}\n$$",
   "metadata": {
    "id": "EXImNPvtADvi",
    "cell_id": "00004-29a7f9ff-346b-4070-a974-b75079aa3589",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### When Should you Normalize the Features?",
   "metadata": {
    "id": "14c25a1d",
    "cell_id": "00005-c004016f-272e-41e6-8dc4-99f00a8e8be0",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "When you have features that have different ranges of values, normalizing these features can be a good practice. \n\nTake an example. If you have two features that have different ranges (say one feature from 1-100, other vary from 5-300), you will to scale them so they have the same range of values. \n\nMore specifically, normalization is a preferrable scaling technique when the data at hand has not a normal or gaussian distribution. If the data's distribution is gaussian, standardization is a preferrable scaling technique. If you don't know the distribution of the data, still, normalization is a good choice at first. \n\nWith that said, when the ML algorithm of choice is either neural network or K-Nearest Neighbors(KNN), normalization is a good choice for these type of algorithms because they don't make any assumption of the input data. ",
   "metadata": {
    "id": "2K8Ky-aW6Sw1",
    "cell_id": "00006-c67e54b0-baa3-42fd-92a0-4a21e2b1c36e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Most popular ML frameworks provide functions to normalize the numerical data. \n\nFor illustration purpose, I will use tips data available in Seaborn. ",
   "metadata": {
    "id": "I67mjU6m56MO",
    "cell_id": "00007-bfd37453-6c42-4ffc-b8d9-71313cc26d73",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "d20d3476",
    "outputId": "ec767f94-6682-4df8-8155-3dc13faef941",
    "cell_id": "00008-b48f8009-aa25-4cab-8dcc-bc6e3314d68e",
    "deepnote_cell_type": "code"
   },
   "source": "from seaborn import load_dataset\n\ntip_data = load_dataset('tips')\ntip_data.head()",
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>total_bill</th>\n      <th>tip</th>\n      <th>sex</th>\n      <th>smoker</th>\n      <th>day</th>\n      <th>time</th>\n      <th>size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>16.99</td>\n      <td>1.01</td>\n      <td>Female</td>\n      <td>No</td>\n      <td>Sun</td>\n      <td>Dinner</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10.34</td>\n      <td>1.66</td>\n      <td>Male</td>\n      <td>No</td>\n      <td>Sun</td>\n      <td>Dinner</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>21.01</td>\n      <td>3.50</td>\n      <td>Male</td>\n      <td>No</td>\n      <td>Sun</td>\n      <td>Dinner</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>23.68</td>\n      <td>3.31</td>\n      <td>Male</td>\n      <td>No</td>\n      <td>Sun</td>\n      <td>Dinner</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24.59</td>\n      <td>3.61</td>\n      <td>Female</td>\n      <td>No</td>\n      <td>Sun</td>\n      <td>Dinner</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   total_bill   tip     sex smoker  day    time  size\n0       16.99  1.01  Female     No  Sun  Dinner     2\n1       10.34  1.66    Male     No  Sun  Dinner     3\n2       21.01  3.50    Male     No  Sun  Dinner     3\n3       23.68  3.31    Male     No  Sun  Dinner     2\n4       24.59  3.61  Female     No  Sun  Dinner     4"
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Let's take all numerical features from the above data. ",
   "metadata": {
    "id": "0c87d399",
    "cell_id": "00009-6de0a1a1-081e-4f18-a848-3ba5b6d8c67d",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f49318c8",
    "cell_id": "00010-997d7e82-6b57-475a-8211-b6c2e00d1cca",
    "deepnote_cell_type": "code"
   },
   "source": "num_feats = tip_data[['total_bill', 'tip', 'size']]",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "For now let's scale those numerical features with Scikit-Learn preprocessing functions. We will use `MinMaxScaler` which scale the data to the range between 0 and 1 by default. If you want a different range, you can change that. ",
   "metadata": {
    "id": "2e806119",
    "cell_id": "00011-ad42d51f-219e-45ef-9c55-1497cf8dabb6",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d63dd722",
    "outputId": "ee8c8547-be58-4408-a0fc-f6793b589675",
    "cell_id": "00012-ab1612d4-5747-4828-8fe6-94c619fd5956",
    "deepnote_cell_type": "code"
   },
   "source": "from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\nnum_scaled = scaler.fit_transform(num_feats)\nnum_scaled[:5]",
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.29157939, 0.00111111, 0.2       ],\n       [0.1522832 , 0.07333333, 0.4       ],\n       [0.3757855 , 0.27777778, 0.4       ],\n       [0.43171345, 0.25666667, 0.2       ],\n       [0.45077503, 0.29      , 0.6       ]])"
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "The output of the scaler is a NumPy array. You can convert it back to a Pandas DataFrame. ",
   "metadata": {
    "id": "2a850c99",
    "cell_id": "00013-1670ec95-6d96-4a2f-b440-a09913f7ab20",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "6a757301",
    "outputId": "b40c142f-78f3-4f53-f665-e56b0fb2df73",
    "cell_id": "00014-13474183-4fa7-45f7-ba55-b49f146d3eee",
    "deepnote_cell_type": "code"
   },
   "source": "import pandas as pd \n\nnum_scaled_df = pd.DataFrame(num_scaled, columns=num_feats.columns)\nnum_scaled_df.head()",
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>total_bill</th>\n      <th>tip</th>\n      <th>size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.291579</td>\n      <td>0.001111</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.152283</td>\n      <td>0.073333</td>\n      <td>0.4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.375786</td>\n      <td>0.277778</td>\n      <td>0.4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.431713</td>\n      <td>0.256667</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.450775</td>\n      <td>0.290000</td>\n      <td>0.6</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   total_bill       tip  size\n0    0.291579  0.001111   0.2\n1    0.152283  0.073333   0.4\n2    0.375786  0.277778   0.4\n3    0.431713  0.256667   0.2\n4    0.450775  0.290000   0.6"
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "As you can see above, all the values are scaled to the values between 0 and 1. ",
   "metadata": {
    "id": "1161d3b5",
    "cell_id": "00015-e7b12a40-6b09-4417-a08e-ede4afeb6d78",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Standardization",
   "metadata": {
    "id": "27354aa2",
    "cell_id": "00016-21e29193-a54e-49af-8fb8-4828ed2900b0",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "In standardization, the numerical features are rescaled to have the 0 mean($u$) and unity standard deviation(std or $\\sigma$ ). \n\nHere is the formula of standardization. Xstd is the standardized feature, X is the feature, $u$ is mean of the feature, and $\\sigma$ is the standard deviation. \n\n$$\nXstd = \\frac {X - u} {\\sigma}\n$$",
   "metadata": {
    "id": "BfCJFRN-Cb2A",
    "cell_id": "00017-0384c729-0ab0-4604-a86e-d947b91cce55",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### When Should you Standardize the Features?\n\nWhen you know that the training data at hand has a normal or gaussian distribution, you should standardize such data. \n\nSome ML algorithms such as Support Vector Machines(with rbf kernel) and linear models expect that the input data to have a normal distribution. \n\nIn most cases, whether you choose normalization or standardization, it won't make much difference, but it can. So, it makes sense to try both especially if you are not sure about the distribution of the data.\n\nHere is how Standardization is implemented in Scikit-Learn.",
   "metadata": {
    "id": "ae039d4b",
    "cell_id": "00018-4fecdf4f-0031-4a29-9abb-54f2f303aaa4",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QNsSSG-SHi6U",
    "outputId": "42e103cd-8a49-4d9f-c73b-1d60eedc049a",
    "cell_id": "00019-9479ebef-b524-477a-9757-c4080a5fb322",
    "deepnote_cell_type": "code"
   },
   "source": "from sklearn.preprocessing import StandardScaler\n\nstd_scaler = StandardScaler()\nnum_std = std_scaler.fit_transform(num_feats)\nnum_std[:5]",
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.31471131, -1.43994695, -0.60019263],\n       [-1.06323531, -0.96920534,  0.45338292],\n       [ 0.1377799 ,  0.36335554,  0.45338292],\n       [ 0.4383151 ,  0.22575414, -0.60019263],\n       [ 0.5407447 ,  0.4430195 ,  1.50695847]])"
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X6JC0b2uLuwq",
    "outputId": "6f2f99fe-bec8-4f8f-e376-f65cde9da0ea",
    "cell_id": "00020-49c8ebf8-370b-46ce-8cd0-be27e4ffe840",
    "deepnote_cell_type": "code"
   },
   "source": "# The mean of each feature in the scaled data\n\nstd_scaler.mean_",
   "outputs": [
    {
     "data": {
      "text/plain": "array([19.78594262,  2.99827869,  2.56967213])"
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eQ5huG8aL-UJ",
    "outputId": "e0df31e0-86e0-40c4-ebc3-23b8e9598e3c",
    "cell_id": "00021-262ef5d5-78cf-4388-bab4-4cc1c3309689",
    "deepnote_cell_type": "code"
   },
   "source": "# Variance of the scaled features\n\nstd_scaler.var_",
   "outputs": [
    {
     "data": {
      "text/plain": "array([78.92813149,  1.90660851,  0.9008835 ])"
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Scaled data has zero mean an unit variance.",
   "metadata": {
    "id": "Gdniw_yyNXVH",
    "cell_id": "00022-bdc2550a-7ff6-4ac0-8cfd-ecfc2683de41",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tfBfcO_CNbZ7",
    "outputId": "eb214052-34d8-4e24-e7fb-3515645549f9",
    "cell_id": "00023-7bcc3afb-1b8f-432f-950e-fb394985a577",
    "deepnote_cell_type": "code"
   },
   "source": "import numpy as np\n\nprint(f'The mean of scaled data: {np.round(num_std.mean(axis=0))}')\nprint(f'The standard deviation of scaled data: {num_std.std(axis=0)}')",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The mean of scaled data: [-0.  0. -0.]\nThe standard deviation of scaled data: [1. 1. 1.]\n"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "94oLxVmCIFBD",
    "outputId": "c73e7131-04c9-4189-8faf-c17c05246d57",
    "cell_id": "00024-96fed2a5-a2fa-4224-aa49-af10b10297cd",
    "deepnote_cell_type": "code"
   },
   "source": "# Converting the scaled values back to datframe \n\nnum_std_scaled_df = pd.DataFrame(num_std, columns=num_feats.columns)\nnum_std_scaled_df.head()",
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>total_bill</th>\n      <th>tip</th>\n      <th>size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.314711</td>\n      <td>-1.439947</td>\n      <td>-0.600193</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.063235</td>\n      <td>-0.969205</td>\n      <td>0.453383</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.137780</td>\n      <td>0.363356</td>\n      <td>0.453383</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.438315</td>\n      <td>0.225754</td>\n      <td>-0.600193</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.540745</td>\n      <td>0.443020</td>\n      <td>1.506958</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   total_bill       tip      size\n0   -0.314711 -1.439947 -0.600193\n1   -1.063235 -0.969205  0.453383\n2    0.137780  0.363356  0.453383\n3    0.438315  0.225754 -0.600193\n4    0.540745  0.443020  1.506958"
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Robust Scaler",
   "metadata": {
    "id": "q_EfBa0kMw6o",
    "cell_id": "00025-12b174a7-fe18-41a9-b2f4-2ece47efbdf6",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "[Robust scaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler) is kind of similar to standardization but is used when the data contains many outliers. \n\nInstead of dropping mean, the median is dropped and the data is scaled to the Interquartile Range(IQR). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).\n\nLike normalization and standardization, Roust scaler is also implemented easily in Scikit-Learn. ",
   "metadata": {
    "id": "oDu_7GCPM2Ne",
    "cell_id": "00026-486503cc-78fa-4942-8936-97163a81b13a",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2tPWb_jPSRy",
    "outputId": "f8f18e7c-7797-4f53-a988-a60471b508f3",
    "cell_id": "00027-316b1225-f4c4-4de6-9761-80d649458dae",
    "deepnote_cell_type": "code"
   },
   "source": "from sklearn.preprocessing import RobustScaler\n\nrob_scaler = RobustScaler()\nnum_rob_scaled = rob_scaler.fit_transform(num_feats)\n\nnum_rob_scaled[:5]",
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.07467532, -1.2096    ,  0.        ],\n       [-0.69155844, -0.7936    ,  1.        ],\n       [ 0.29823748,  0.384     ,  1.        ],\n       [ 0.54591837,  0.2624    ,  0.        ],\n       [ 0.63033395,  0.4544    ,  2.        ]])"
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "By scaling the data with Robust Scaler, the median of the resulting values will have a median of zero. ",
   "metadata": {
    "id": "eGKJqjlLRJ0_",
    "cell_id": "00028-a73d4744-5c8c-4250-b550-7b6764c43a1b",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A6dBrs_wQJ4M",
    "outputId": "fae7bfcc-3fb3-42f9-c723-08fcf773993a",
    "cell_id": "00029-af3b7661-9be4-4d7e-867b-55c6dffa3bd9",
    "deepnote_cell_type": "code"
   },
   "source": "print(f'The median of scaled data: {np.round(np.median(num_rob_scaled, axis=0))}')",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The median of scaled data: [-0.  0.  0.]\n"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Final Notes",
   "metadata": {
    "id": "zf59BqHWS9aR",
    "cell_id": "00030-a8470543-a7c0-49ea-9aa5-a181c2b6691e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Scaling the input data before feeding it to a machine learning model is always a good practice. \n\nHere are the punchlines: \n\n* **Scaling** the features helps the model to converge faster.\n* **Normalization** is scaling the data to be between 0 and 1. It is preferred when the data has not a normal distribution\n* **Standardization** is scaling the data to have 0 mean and unit standard deviation. It is preferred when the data has a normal or gaussian distribution.\n* **Robust scaling** technique is used if the data has many outliers. \n* In most cases, the choice of scaling technique won't make much difference (or it can). **Try all of them** and see what work best with your data.\n* Only the features are scaled. The labels should not be scaled. \n* Make sure to not fit the scaler on test data. Only transfom. \n\n```\nDon't: scaler.fit_transfrom(X_test)\nDo: scaler.transform(X_test)\n```",
   "metadata": {
    "id": "m36tAB_zTCIx",
    "cell_id": "00031-ce499ef1-fae1-44dc-9518-046a1d0e523e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Futher Learning\n\n* Blog: https://jeande.medium.com/the-ultimate-and-practical-guide-on-feature-scaling-d03fbe2cb25e\n* http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html\n* https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling\n",
   "metadata": {
    "id": "7a6de3d9",
    "cell_id": "00032-26aa3d1e-8081-44ec-859b-b5455ea9a264",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=dd96149a-b450-404d-848f-a6a4961acb1a' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4.4 Feature Scaling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('tensor': conda)",
   "language": "python",
   "name": "python3710jvsc74a57bd034ac5db714c5906ee087fcf6e2d00ee4febf096586592b6ba3662ed3b7e7a5f6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "deepnote_notebook_id": "9be292e7-51a4-4df6-b00e-540cdbd8ed96",
  "deepnote": {},
  "deepnote_execution_queue": []
 }
}